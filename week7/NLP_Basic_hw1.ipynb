{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['the woman is a wise queen',\n",
    "          'the man is a wise president',\n",
    "          'she is a pretty woman',\n",
    "          'he is a strong man',\n",
    "          'she is still young',\n",
    "          'he is very old',\n",
    "          'he is the current president of US',\n",
    "          'the prince is a son of the king',\n",
    "          'the princess is a daughter of the king',\n",
    "          'a prince is a young man',\n",
    "          'a princess is a young woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q1) 주석 부분에 function1의 목적을 쓰고, 그 주된 목적에 맞게 function1의 이름 변경하기\n",
    "'''\n",
    "function1의 목적 : target word와 neighbor word 매칭\n",
    "'''\n",
    "def w2v_input(corpus):    \n",
    "    sentences = []\n",
    "    for sentence in corpus:\n",
    "        sentences.append(sentence.split())\n",
    "\n",
    "    w_size = 2 # q2) s의 의미를 적고 그에 맞게 변수명 변경하기\n",
    "    '''\n",
    "    s의 의미 : window size\n",
    "    '''\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        for idx, word in enumerate(sentence):\n",
    "            for neighbor in sentence[max(idx - w_size, 0) : min(idx + w_size, len(sentence)) + 1] : \n",
    "                if neighbor != word:\n",
    "                    data.append([word, neighbor])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = w2v_input(corpus)\n",
    "df = pd.DataFrame(data, columns = ['input', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>woman</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>woman</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>woman</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   input  label\n",
       "0    the  woman\n",
       "1    the     is\n",
       "2  woman    the\n",
       "3  woman     is\n",
       "4  woman      a"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q3) 주석 부분에 function2의 목적을 쓰고, 그 목적에 맞게 function2의 이름 변경하기\n",
    "'''\n",
    "function2의 목적 : corpus에 등장하는 모든 단어의 집합을 만드는 함수 (중복 x)\n",
    "'''\n",
    "def word_set(corpus):\n",
    "    words = []\n",
    "    for text in corpus:\n",
    "        for word in text.split(' '):\n",
    "            words.append(word)\n",
    "    words = set(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'US',\n",
       " 'a',\n",
       " 'current',\n",
       " 'daughter',\n",
       " 'he',\n",
       " 'is',\n",
       " 'king',\n",
       " 'man',\n",
       " 'of',\n",
       " 'old',\n",
       " 'president',\n",
       " 'pretty',\n",
       " 'prince',\n",
       " 'princess',\n",
       " 'queen',\n",
       " 'she',\n",
       " 'son',\n",
       " 'still',\n",
       " 'strong',\n",
       " 'the',\n",
       " 'very',\n",
       " 'wise',\n",
       " 'woman',\n",
       " 'young'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_set(corpus)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q4) 주석 부분에 function3의 목적을 쓰고, 그 목적에 맞게 function3과 d의 이름을 변경하기\n",
    "'''\n",
    "function3의 목적 : 각 단어들을 key로 나열순서 index를 value에 저장하는 dict를 만드는 함수\n",
    "'''\n",
    "def word_indexing(words):\n",
    "    word_index = {}\n",
    "    for i,word in enumerate(words):\n",
    "        word_index[word] = i\n",
    "    return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'man': 0,\n",
       " 'old': 1,\n",
       " 'he': 2,\n",
       " 'still': 3,\n",
       " 'current': 4,\n",
       " 'is': 5,\n",
       " 'the': 6,\n",
       " 'princess': 7,\n",
       " 'queen': 8,\n",
       " 'strong': 9,\n",
       " 'very': 10,\n",
       " 'woman': 11,\n",
       " 'of': 12,\n",
       " 'king': 13,\n",
       " 'son': 14,\n",
       " 'wise': 15,\n",
       " 'daughter': 16,\n",
       " 'a': 17,\n",
       " 'pretty': 18,\n",
       " 'young': 19,\n",
       " 'US': 20,\n",
       " 'prince': 21,\n",
       " 'president': 22,\n",
       " 'she': 23}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = word_indexing(words)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# q5) 주석 부분에 function4의 목적을 쓰고, 그 목적에 맞게 function4와 ohe의 이름 변경하기\n",
    "'''\n",
    "function4의 목적 : one hot encoding 함수\n",
    "'''\n",
    "def one_hot_encoding(word_index, ONE_HOT_DIM):\n",
    "    ohe = np.zeros(ONE_HOT_DIM)\n",
    "    ohe[word_index] = 1\n",
    "    return ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSkip Gram\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q6) Word2Vec을 tensorflow로 구현한 코드에서 ? 부분을 올바르게 채워넣기\n",
    "# q7) 여기서 구현한 Word2Vec의 아키텍쳐는 CBOW or Skip Gram ?\n",
    "'''\n",
    "Skip Gram\n",
    "'''\n",
    "# cross entropy 참고\n",
    "# https://ratsgo.github.io/deep%20learning/2017/10/02/softmax/\n",
    "# https://kevinthegrey.tistory.com/123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "ONE_HOT_DIM = len(words)\n",
    "\n",
    "X = [] # input word\n",
    "Y = [] # target word\n",
    "\n",
    "for x, y in zip(df['input'], df['label']):\n",
    "    X.append(one_hot_encoding(d[x], ONE_HOT_DIM))\n",
    "    Y.append(one_hot_encoding(d[y], ONE_HOT_DIM))\n",
    "    \n",
    "    # convert X,Y to numpy arrays\n",
    "X_train = np.asarray(X)\n",
    "Y_train = np.asarray(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "ONE_HOT_DIM = len(words)\n",
    "\n",
    "X = [] # input word\n",
    "Y = [] # target word\n",
    "\n",
    "for x, y in zip(df['input'], df['label']):\n",
    "    X.append(one_hot_encoding(d[x], ONE_HOT_DIM))\n",
    "    Y.append(one_hot_encoding(d[y], ONE_HOT_DIM))\n",
    "\n",
    "# convert X,Y to numpy arrays\n",
    "X_train = np.asarray(X)\n",
    "Y_train = np.asarray(Y)\n",
    "\n",
    "# placeholders for X_train and Y_train\n",
    "x = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
    "\n",
    "# embedding dimension\n",
    "EMBEDDING_DIM = 2\n",
    "\n",
    "# hidden layer : represent word vector eventually\n",
    "W1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM]))\n",
    "b1 = tf.Variable(tf.random_normal([1]))\n",
    "hidden_layer = tf.add(tf.matmul(x,W1), b1)\n",
    "\n",
    "# output layer\n",
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM]))\n",
    "b2 = tf.Variable(tf.random_normal([1]))\n",
    "output = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2))\n",
    "\n",
    "# loss function : cross entropy\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(output), axis=[1]))\n",
    "\n",
    "# training\n",
    "train = tf.train.GradientDescentOptimizer(0.05).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 loss is :  6.7417226\n",
      "iteration 2000 loss is :  2.5378864\n",
      "iteration 4000 loss is :  2.47047\n",
      "iteration 6000 loss is :  2.442094\n",
      "iteration 8000 loss is :  2.422756\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n",
    "\n",
    "iteration = 10000\n",
    "for i in range(iteration):\n",
    "    # input : X_train which is one hot encoded word\n",
    "    # label : Y_train which is one hot encoded neighbor word\n",
    "    sess.run(train, feed_dict={x: X_train, y_label: Y_train})\n",
    "    if i % 2000 == 0:\n",
    "        print('iteration '+ str(i) +' loss is : ', sess.run(loss, feed_dict={x: X_train, y_label: Y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.9886237 ,  0.5578252 ],\n",
       "       [-1.0275269 , -0.59022737],\n",
       "       [-1.6285518 , -1.4305437 ],\n",
       "       [-1.2066599 ,  0.905879  ],\n",
       "       [-0.98765403, -3.1654143 ],\n",
       "       [-0.8895926 ,  0.15903425],\n",
       "       [-0.48015952, -1.4624747 ],\n",
       "       [-2.7858648 , -1.3706934 ],\n",
       "       [-1.2663746 , -0.38706785],\n",
       "       [-1.5755341 , -1.0651104 ],\n",
       "       [-0.5117104 ,  0.18440795],\n",
       "       [-1.9315703 ,  0.46140575],\n",
       "       [-0.19389528, -1.0478638 ],\n",
       "       [-1.031911  , -3.2740448 ],\n",
       "       [-1.2380123 , -2.4367032 ],\n",
       "       [-0.94369256, -1.6140618 ],\n",
       "       [-1.3060849 , -2.6613817 ],\n",
       "       [-0.56807786, -0.5422425 ],\n",
       "       [-1.5044446 , -0.84225255],\n",
       "       [-1.3898277 , -0.90263087],\n",
       "       [ 0.50633895, -2.7538352 ],\n",
       "       [-2.8421426 , -1.4314857 ],\n",
       "       [-0.31798965, -1.018264  ],\n",
       "       [-1.6272169 , -1.0346696 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the hidden layer (W1 + b1) -> look up table\n",
    "vectors = sess.run(W1 + b1)\n",
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>man</td>\n",
       "      <td>-1.988624</td>\n",
       "      <td>0.557825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>old</td>\n",
       "      <td>-1.027527</td>\n",
       "      <td>-0.590227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he</td>\n",
       "      <td>-1.628552</td>\n",
       "      <td>-1.430544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>still</td>\n",
       "      <td>-1.206660</td>\n",
       "      <td>0.905879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>current</td>\n",
       "      <td>-0.987654</td>\n",
       "      <td>-3.165414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>is</td>\n",
       "      <td>-0.889593</td>\n",
       "      <td>0.159034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the</td>\n",
       "      <td>-0.480160</td>\n",
       "      <td>-1.462475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>princess</td>\n",
       "      <td>-2.785865</td>\n",
       "      <td>-1.370693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>queen</td>\n",
       "      <td>-1.266375</td>\n",
       "      <td>-0.387068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>strong</td>\n",
       "      <td>-1.575534</td>\n",
       "      <td>-1.065110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>very</td>\n",
       "      <td>-0.511710</td>\n",
       "      <td>0.184408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>woman</td>\n",
       "      <td>-1.931570</td>\n",
       "      <td>0.461406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>of</td>\n",
       "      <td>-0.193895</td>\n",
       "      <td>-1.047864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>king</td>\n",
       "      <td>-1.031911</td>\n",
       "      <td>-3.274045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>son</td>\n",
       "      <td>-1.238012</td>\n",
       "      <td>-2.436703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>wise</td>\n",
       "      <td>-0.943693</td>\n",
       "      <td>-1.614062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>daughter</td>\n",
       "      <td>-1.306085</td>\n",
       "      <td>-2.661382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>a</td>\n",
       "      <td>-0.568078</td>\n",
       "      <td>-0.542243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pretty</td>\n",
       "      <td>-1.504445</td>\n",
       "      <td>-0.842253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>young</td>\n",
       "      <td>-1.389828</td>\n",
       "      <td>-0.902631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>US</td>\n",
       "      <td>0.506339</td>\n",
       "      <td>-2.753835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>prince</td>\n",
       "      <td>-2.842143</td>\n",
       "      <td>-1.431486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>president</td>\n",
       "      <td>-0.317990</td>\n",
       "      <td>-1.018264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>she</td>\n",
       "      <td>-1.627217</td>\n",
       "      <td>-1.034670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word        x1        x2\n",
       "0         man -1.988624  0.557825\n",
       "1         old -1.027527 -0.590227\n",
       "2          he -1.628552 -1.430544\n",
       "3       still -1.206660  0.905879\n",
       "4     current -0.987654 -3.165414\n",
       "5          is -0.889593  0.159034\n",
       "6         the -0.480160 -1.462475\n",
       "7    princess -2.785865 -1.370693\n",
       "8       queen -1.266375 -0.387068\n",
       "9      strong -1.575534 -1.065110\n",
       "10       very -0.511710  0.184408\n",
       "11      woman -1.931570  0.461406\n",
       "12         of -0.193895 -1.047864\n",
       "13       king -1.031911 -3.274045\n",
       "14        son -1.238012 -2.436703\n",
       "15       wise -0.943693 -1.614062\n",
       "16   daughter -1.306085 -2.661382\n",
       "17          a -0.568078 -0.542243\n",
       "18     pretty -1.504445 -0.842253\n",
       "19      young -1.389828 -0.902631\n",
       "20         US  0.506339 -2.753835\n",
       "21     prince -2.842143 -1.431486\n",
       "22  president -0.317990 -1.018264\n",
       "23        she -1.627217 -1.034670"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_df = pd.DataFrame(vectors, columns = ['x1', 'x2'])\n",
    "w2v_df['word'] = words\n",
    "w2v_df = w2v_df[['word', 'x1', 'x2']]\n",
    "w2v_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for word, x1, x2 in zip(w2v_df['word'], w2v_df['x1'], w2v_df['x2']):\n",
    "    ax.annotate(word, (x1,x2 ))\n",
    "    \n",
    "PADDING = 1.0\n",
    "x1_axis_min = np.amin(vectors, axis=0)[0] - PADDING\n",
    "x2_axis_min = np.amin(vectors, axis=0)[1] - PADDING\n",
    "x1_axis_max = np.amax(vectors, axis=0)[0] + PADDING\n",
    "x2_axis_max = np.amax(vectors, axis=0)[1] + PADDING\n",
    " \n",
    "plt.xlim(x1_axis_min,x1_axis_max)\n",
    "plt.ylim(x2_axis_min,x2_axis_max)\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
