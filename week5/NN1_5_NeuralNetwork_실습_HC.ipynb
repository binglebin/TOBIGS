{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(X, W): #데이터와 가중치의 내적 함수\n",
    "    H = np.dot(X, W)\n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(H): # 0과 1 사이의 값을 반납하는 sigmoid 함수\n",
    "    p = 1 / (1 + np.exp(-H))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1, 2]) # 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([2, 3]) # 가충치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "0.9996646498695336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nsigmoid(H) = 1에 가까운 값\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = linear(X, W)\n",
    "print(H) \n",
    "p = sigmoid(H)\n",
    "print(p)\n",
    "'''\n",
    "sigmoid(H) = 1에 가까운 값\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([-4, -3]) # 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10\n",
      "4.5397868702434395e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nsigmoid(H) = 0에 가까운 값\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = linear(X, W)\n",
    "print(H)\n",
    "p = sigmoid(H)\n",
    "print(p)\n",
    "'''\n",
    "sigmoid(H) = 0에 가까운 값\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"data\"]\n",
    "y = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.shape)\n",
    "x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.params = {} # list initialization\n",
    "        self.params['W'] = 0.0001 * np.random.randn(4, 3) # weight initialization (4,3)\n",
    "        self.params['b'] = np.ones(3) # bias initialization (3)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        W = self.params['W']\n",
    "        b = self.params['b']\n",
    "\n",
    "        h = np.dot(X, W) + b # (3 ,1) hidden layer 만들기\n",
    "        a = np.exp(h)\n",
    "        #stable_a = np.exp(h - np.max(h, axis = 1).reshape(-1,1))\n",
    "        p = a/np.sum(a, axis = 1).reshape(-1,1) # soft max 함수\n",
    "        return p\n",
    "    \n",
    "    def loss(self, X, T):\n",
    "        \n",
    "        p = self.forward(X) #softmax 변환된 확률\n",
    "        \n",
    "        n = T.shape[0] # 데이터 수\n",
    "        log_likelihood = 0 \n",
    "        log_likelihood -= np.log(p[np.arange(n), T]).sum() # 실제 라벨에 대응되는 softmax 확률들의 log_likelyhood\n",
    "        Loss = log_likelihood / n # n개의 loss값이 더해진 값을 n으로 나눠줌\n",
    "\n",
    "        return Loss\n",
    "    \n",
    "    def accuracy(self, X, T):\n",
    "        p = self.forward(X) #예측\n",
    "        predict = np.argmax(p, axis = 1) #예측 결과 index 1darray 로 출력 \n",
    "        \n",
    "        return 1 - np.count_nonzero(predict - T)/len(T) #전체 값 분의 predict-T가 0인 값\n",
    "        \n",
    "    def gradient(self, X, T, learning_rate = 0.0001):\n",
    "        \n",
    "        p = self.forward(X) #softmax 변환된 확률\n",
    "        \n",
    "        t = np.zeros((T.shape[0], 3)) \n",
    "        t[np.arange(T.shape[0]), T] = 1\n",
    "        #t는 인덱스 레이블 T를 One hot 벡터로 바꾼 것\n",
    "        \n",
    "        dp = p.copy() #list copy 미분값\n",
    "        dp[np.arange(len(T)), T] -= 1 # loss 함수를 ai로 미분하면 pi - yi\n",
    "        #목적함수에 대한 가중치 미분값을 담을 zero array 생성\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W'] = np.zeros((4, 3)) # weight initialization (4,3)\n",
    "        grads['b'] = np.zeros(3) # bias initialization (3)\n",
    "        #목적함수에 대한 가중치 미분값 합 구하기\n",
    "        grads['W'] = (1/len(T)) * np.dot(X.T, p-t) #loss 함수를 w로 미분하면 dot(X.T, p-t)\n",
    "        grads['b'] = (1/len(T)) * np.sum(p-t, axis = 0) #loss 함수를 b로 미분하면 p-t\n",
    "        #p-t 대신 dp 사용 가능\n",
    "        \n",
    "        self.params['W'] -= learning_rate * grads['W'] # 기울기의 반대반향으로 W 조정\n",
    "        self.params['b'] -= learning_rate * grads['b'] # 기울기의 반대반향으로 b 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = Softmax()\n",
    "# softmax class 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 학습중입니다.\n",
      "Accuracy :  0.41333333333333333\n",
      "Loss :      1.098325468410954\n",
      "[[ 6.26061692e-05 -6.24096983e-05  1.03096840e-04]\n",
      " [ 1.41890272e-05 -1.30413724e-04 -1.72694373e-04]\n",
      " [-2.39860699e-04  1.44777533e-05  7.38216235e-05]\n",
      " [-5.03145348e-05  2.59013044e-05  5.27206251e-05]]\n",
      "1000 번째 학습중입니다.\n",
      "Accuracy :  0.33999999999999997\n",
      "Loss :      1.0199954646749299\n",
      "[[-0.00311427  0.00062693  0.00259063]\n",
      " [ 0.02405756 -0.01073946 -0.01360702]\n",
      " [-0.05802985  0.01471916  0.04315912]\n",
      " [-0.02546577  0.00354036  0.02195372]]\n",
      "2000 번째 학습중입니다.\n",
      "Accuracy :  0.6666666666666667\n",
      "Loss :      0.9645286547874616\n",
      "[[ 0.00870318 -0.00037711 -0.00822278]\n",
      " [ 0.05412849 -0.02186168 -0.03255572]\n",
      " [-0.10210576  0.02754727  0.07440693]\n",
      " [-0.04596722  0.00636237  0.03963315]]\n",
      "3000 번째 학습중입니다.\n",
      "Accuracy :  0.6666666666666667\n",
      "Loss :      0.9162939939259718\n",
      "[[ 0.02164534 -0.00160764 -0.01993441]\n",
      " [ 0.08324819 -0.03269643 -0.05084067]\n",
      " [-0.14203923  0.03937872  0.10250895]\n",
      " [-0.06470159  0.00876673  0.05596317]]\n",
      "4000 번째 학습중입니다.\n",
      "Accuracy :  0.6666666666666667\n",
      "Loss :      0.8741277154348759\n",
      "[[ 0.03378745 -0.00256038 -0.03112378]\n",
      " [ 0.11051429 -0.04296112 -0.06784209]\n",
      " [-0.17931208  0.05050364  0.12865688]\n",
      " [-0.08217592  0.01084318  0.07136105]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5000):\n",
    "    softmax.gradient(x, y)\n",
    "    \n",
    "    if i % 1000 == 0: \n",
    "        print(i, \"번째 학습중입니다.\")\n",
    "        print(\"Accuracy : \", softmax.accuracy(x, y))\n",
    "        print(\"Loss :     \", softmax.loss(x, y))\n",
    "        print(softmax.params['W'])\n",
    "        \n",
    "#5000번 학습 1000번마다 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
